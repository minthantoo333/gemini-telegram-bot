import logging
import os
import asyncio
import glob
import subprocess
import torch
import pysrt
import math
import shutil
import reÂ 

# User Data
user_prefs = {}
user_modes = {}Â 
chat_histories = {}
user_last_active = {} # ğŸ•’ Last Active Time á€™á€¾á€á€ºá€–á€­á€¯á€·

# Audio Processing
from pydub import AudioSegment, effects
from pydub.silence import detect_leading_silence # Added for trimming silence
import edge_tts

# Telegram
from telegram import Update, InlineKeyboardButton, InlineKeyboardMarkup, BotCommand
from telegram.ext import ApplicationBuilder, ContextTypes, MessageHandler, CommandHandler, CallbackQueryHandler, filters

# AI Tools
from google import genai
from google.genai import types
from faster_whisper import WhisperModel

# --- âš™ï¸ CONFIGURATION ---
TG_TOKEN = os.getenv("TG_TOKEN")
GEMINI_KEY = os.getenv("GEMINI_KEY")

if not TG_TOKEN or not GEMINI_KEY:
Â  Â  print("âŒ ERROR: API Keys are missing! Set them in your environment variables.")
Â  Â  exit()

# --- ğŸ—£ï¸ VOICE LIBRARY ---
VOICE_LIB = {
Â  Â  "ğŸ‡²ğŸ‡² Burmese (Male)": "my-MM-ThihaNeural",
Â  Â  "ğŸ‡²ğŸ‡² Burmese (Female)": "my-MM-NularNeural",
Â  Â  "ğŸ‡ºğŸ‡¸ Remy (Multi)": "en-US-RemyMultilingualNeural",
Â  Â  "ğŸ‡®ğŸ‡¹ Giuseppe (Multi)": "it-IT-GiuseppeMultilingualNeural",
Â  Â  "ğŸ‡ºğŸ‡¸ Brian (Male)": "en-US-BrianNeural",
Â  Â  "ğŸ‡ºğŸ‡¸ Andrew (Male)": "en-US-AndrewNeural"
}

# --- ğŸ“ PROMPTS ---
SRT_RULES = """
**FORMATTING INSTRUCTIONS (STRICT):**
1. The input is an **SRT Subtitle File**.
2. **OUTPUT FORMAT:** You MUST return a valid SRT file.
3. **TIMESTAMPS:** Do NOT change, shift, or remove any timestamps.Â 
4. **SEQUENCE NUMBERS:** Preserve exact sequence.
5. **TRANSLATION:** Translate text to natural Burmese.
6. **LOAN WORDS:** Phonetic spelling for English terms (e.g., CEO -> á€…á€®á€¡á€®á€¸á€¡á€­á€¯).
"""

BURMESE_STYLE = """
Role: Professional Video Narrator (Burmese).
Style: Natural, engaging, clear narration.
No 'á€•á€±á€«á€·' (pout) at end of sentences.
Translate naturally as a continuous story, not robotic word-by-word.
"""

DEFAULT_PROMPTS = {
Â  Â  "burmese": BURMESE_STYLE,
Â  Â  "rephrase": "Rephrase this English text to be more clear, natural, and reliable."
}

# Folders
BASE_FOLDERS = ["downloads", "temp"]
for f in BASE_FOLDERS:
Â  Â  os.makedirs(f, exist_ok=True)

# User Data
user_prefs = {}
user_modes = {}Â 
chat_histories = {}Â 

# --- ğŸ› ï¸ HELPER FUNCTIONS ---
def get_user_state(user_id):
Â  Â  if user_id not in user_prefs:
Â  Â  Â  Â  user_prefs[user_id] = {
Â  Â  Â  Â  Â  Â  "transcribe_engine": "whisper",Â 
Â  Â  Â  Â  Â  Â  "dub_voice": "my-MM-ThihaNeural",Â 
Â  Â  Â  Â  Â  Â  "custom_prompts": {}Â 
Â  Â  Â  Â  }
Â  Â  return user_prefs[user_id]

def get_active_prompt(user_id, key):
Â  Â  state = get_user_state(user_id)
Â  Â  custom = state.get("custom_prompts", {}).get(key)
Â  Â  return custom if custom else DEFAULT_PROMPTS[key]

def get_paths(user_id):
Â  Â  return {
Â  Â  Â  Â  "input": f"downloads/{user_id}_input.mp4",
Â  Â  Â  Â  "audio": f"downloads/{user_id}_audio.mp3",
Â  Â  Â  Â  "srt": f"downloads/{user_id}_subs.srt",
Â  Â  Â  Â  "txt": f"downloads/{user_id}_transcript.txt",
Â  Â  Â  Â  "trans_result": f"downloads/{user_id}_translated",
Â  Â  Â  Â  "dub_audio": f"downloads/{user_id}_dubbed.mp3"
Â  Â  }

def clean_temp(user_id):
Â  Â  p = get_paths(user_id)
Â  Â  if os.path.exists(p['input']): os.remove(p['input'])
Â  Â  for f in glob.glob(f"temp/{user_id}_chunk_*.mp3"):
Â  Â  Â  Â  try: os.remove(f)
Â  Â  Â  Â  except: pass

def wipe_user_data(user_id):
Â  Â  for f in glob.glob(f"downloads/{user_id}_*"):
Â  Â  Â  Â  try: os.remove(f)
Â  Â  Â  Â  except: pass
Â  Â  clean_temp(user_id)
Â  Â  if user_id in user_prefs: del user_prefs[user_id]
Â  Â  if user_id in user_modes: del user_modes[user_id]
Â  Â  if user_id in chat_histories: del chat_histories[user_id]

async def send_copyable_message(chat_id, bot, text):
Â  Â  if not text: return
Â  Â  MAX_LEN = 4000
Â  Â  safe_text = text.replace("`", "'")Â 
Â  Â  for i in range(0, len(safe_text), MAX_LEN):
Â  Â  Â  Â  chunk = safe_text[i:i+MAX_LEN]
Â  Â  Â  Â  try:
Â  Â  Â  Â  Â  Â  await bot.send_message(chat_id=chat_id, text=f"```\n{chunk}\n```", parse_mode='Markdown')
Â  Â  Â  Â  except Exception as e:
Â  Â  Â  Â  Â  Â  print(f"Message Send Error: {e}")

# --- ğŸ”Š AUDIO POST-PROCESSING ---
def trim_silence(audio_segment, silence_thresh=-40.0, chunk_size=10):
Â  Â  """Trims silence from the beginning and end of an audio segment."""
Â  Â  if len(audio_segment) < 100: Â # Skip if too short
Â  Â  Â  Â  return audio_segment
Â  Â  Â  Â Â 
Â  Â  start_trim = detect_leading_silence(audio_segment, silence_threshold=silence_thresh, chunk_size=chunk_size)
Â  Â  end_trim = detect_leading_silence(audio_segment.reverse(), silence_threshold=silence_thresh, chunk_size=chunk_size)
Â  Â Â 
Â  Â  duration = len(audio_segment)
Â  Â  trimmed = audio_segment[start_trim:duration-end_trim]
Â  Â  return trimmed

def make_audio_crisp(audio_segment):
Â  Â  """Applies filters to make voice sound sharper and clearer."""
Â  Â  clean_audio = audio_segment.high_pass_filter(200)
Â  Â  high_freqs = clean_audio.high_pass_filter(2000)
Â  Â  # Slightly reduced boost to prevent harshness
Â  Â  crisp_audio = clean_audio.overlay(high_freqs - 4)Â 
Â  Â  final_audio = effects.normalize(crisp_audio)
Â  Â  return final_audio

# --- ğŸ¬ DUBBING ENGINE (HYBRID: NATURAL + SYNCED) ---
async def generate_dubbing(user_id, srt_path, output_path, voice):
Â  Â  """
Â  Â  Hybrid Approach:
Â  Â  1. Starts with Voicertool-like settings (+10% speed, -2Hz pitch).
Â  Â  2. CHECKS duration. If audio is too long, speeds it up GENTLY to fit.
Â  Â  3. Maintains sync by adding silence only when necessary (large gaps).
Â  Â  """
Â  Â  print(f"ğŸ¬ Starting Dubbing (Synced + Natural) for {user_id}...")
Â  Â  try:
Â  Â  Â  Â  subs = pysrt.open(srt_path)
Â  Â  Â  Â  final_audio = AudioSegment.empty()
Â  Â  Â  Â  current_timeline_ms = 0
Â  Â  Â  Â Â 
Â  Â  Â  Â  # --- BASE SETTINGS ---
Â  Â  Â  Â  # Start with a comfortable speed that matches Voicertool
Â  Â  Â  Â  BASE_RATE_VAL = 10 # +10%
Â  Â  Â  Â  PITCH_VAL = "-2Hz"

Â  Â  Â  Â  for i, sub in enumerate(subs):
Â  Â  Â  Â  Â  Â  start_ms = (sub.start.hours * 3600 + sub.start.minutes * 60 + sub.start.seconds) * 1000 + sub.start.milliseconds
Â  Â  Â  Â  Â  Â  end_ms = (sub.end.hours * 3600 + sub.end.minutes * 60 + sub.end.seconds) * 1000 + sub.end.milliseconds
Â  Â  Â  Â  Â  Â  allowed_duration_ms = end_ms - start_ms
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  text = sub.text.replace("\n", " ").strip()
Â  Â  Â  Â  Â  Â  if not text: continueÂ 

Â  Â  Â  Â  Â  Â  # --- 1. SYNC CHECK (Wait for start time) ---
Â  Â  Â  Â  Â  Â  # If the previous audio finished EARLY, we must wait for this subtitle's start time.
Â  Â  Â  Â  Â  Â  # Otherwise, the audio will drift and happen too soon.
Â  Â  Â  Â  Â  Â  if start_ms > current_timeline_ms:
Â  Â  Â  Â  Â  Â  Â  Â  gap = start_ms - current_timeline_ms
Â  Â  Â  Â  Â  Â  Â  Â  # Only fill gap if it's significant (>100ms) to avoid micro-stutters
Â  Â  Â  Â  Â  Â  Â  Â  if gap > 100:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  final_audio += AudioSegment.silent(duration=gap)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  current_timeline_ms += gap

Â  Â  Â  Â  Â  Â  # --- 2. GENERATE (First Pass) ---
Â  Â  Â  Â  Â  Â  temp_filename = f"temp/{user_id}_chunk_{i}.mp3"
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  # Start with natural +10% speed
Â  Â  Â  Â  Â  Â  communicate = edge_tts.Communicate(text, voice, rate=f"+{BASE_RATE_VAL}%", pitch=PITCH_VAL)
Â  Â  Â  Â  Â  Â  await communicate.save(temp_filename)
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  segment = AudioSegment.from_file(temp_filename)
Â  Â  Â  Â  Â  Â  segment = trim_silence(segment, silence_thresh=-40.0, chunk_size=5)

Â  Â  Â  Â  Â  Â  # --- 3. DURATION FIT (The Fix) ---
Â  Â  Â  Â  Â  Â  # Check if natural voice is too long for the timestamp
Â  Â  Â  Â  Â  Â  current_len = len(segment)
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  if current_len > allowed_duration_ms:
Â  Â  Â  Â  Â  Â  Â  Â  # Calculate how much faster we need to be
Â  Â  Â  Â  Â  Â  Â  Â  ratio = current_len / allowed_duration_ms
Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  # Calculate new percentage needed (e.g., if ratio is 1.2, we need +20% MORE)
Â  Â  Â  Â  Â  Â  Â  Â  # We add this to our base rate of 10
Â  Â  Â  Â  Â  Â  Â  Â  extra_speed_needed = (ratio - 1) * 100
Â  Â  Â  Â  Â  Â  Â  Â  new_rate = int(BASE_RATE_VAL + extra_speed_needed + 5) # +5 buffer
Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  # CAP the speed so it doesn't sound crazy (Max +50%)
Â  Â  Â  Â  Â  Â  Â  Â  if new_rate > 50: new_rate = 50
Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  # Re-generate with faster speed
Â  Â  Â  Â  Â  Â  Â  Â  communicate = edge_tts.Communicate(text, voice, rate=f"+{new_rate}%", pitch=PITCH_VAL)
Â  Â  Â  Â  Â  Â  Â  Â  await communicate.save(temp_filename)
Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  # Load and Trim again
Â  Â  Â  Â  Â  Â  Â  Â  segment = AudioSegment.from_file(temp_filename)
Â  Â  Â  Â  Â  Â  Â  Â  segment = trim_silence(segment)

Â  Â  Â  Â  Â  Â  # --- 4. CRISP FILTER ---
Â  Â  Â  Â  Â  Â  segment = make_audio_crisp(segment)
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  # --- 5. APPEND ---
Â  Â  Â  Â  Â  Â  final_audio += segment
Â  Â  Â  Â  Â  Â  current_timeline_ms += len(segment)
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  if os.path.exists(temp_filename): os.remove(temp_filename)

Â  Â  Â  Â  final_audio.export(output_path, format="mp3")
Â  Â  Â  Â  return True, None

Â  Â  except Exception as e:
Â  Â  Â  Â  return False, str(e)

# --- Ensure these helpers are present ---
def trim_silence(audio_segment, silence_thresh=-40.0, chunk_size=5):
Â  Â  if len(audio_segment) < 100: return audio_segment
Â  Â  start_trim = detect_leading_silence(audio_segment, silence_threshold=silence_thresh, chunk_size=chunk_size)
Â  Â  end_trim = detect_leading_silence(audio_segment.reverse(), silence_threshold=silence_thresh, chunk_size=chunk_size)
Â  Â  duration = len(audio_segment)
Â  Â  return audio_segment[start_trim:duration-end_trim]

def make_audio_crisp(audio_segment):
Â  Â  clean = audio_segment.high_pass_filter(150)
Â  Â  return effects.normalize(clean)

# --- ğŸ§  ENGINES ---
def run_whisper(audio_path, srt_path, txt_path):
Â  Â  print(f"ğŸ™ï¸ [Whisper] Processing with Sentence Splitting...")
Â  Â  try:
Â  Â  Â  Â  device = "cuda" if torch.cuda.is_available() else "cpu"
Â  Â  Â  Â  compute_type = "float16" if device == "cuda" else "int8"
Â  Â  Â  Â Â 
Â  Â  Â  Â  # Load Model
Â  Â  Â  Â  model = WhisperModel("small", device=device, compute_type=compute_type)
Â  Â  Â  Â Â 
Â  Â  Â  Â  # 1. word_timestamps=True á€€ á€¡á€›á€±á€¸á€¡á€€á€¼á€®á€¸á€†á€¯á€¶á€¸á€•á€«
Â  Â  Â  Â  # á€’á€«á€™á€¾ á€…á€€á€¬á€¸á€œá€¯á€¶á€¸á€á€…á€ºá€œá€¯á€¶á€¸á€á€»á€„á€ºá€¸á€…á€® á€˜á€šá€ºá€¡á€á€»á€­á€”á€ºá€…á€•á€¼á€®á€¸ á€˜á€šá€ºá€¡á€á€»á€­á€”á€ºá€†á€¯á€¶á€¸á€œá€² á€á€­á€™á€¾á€¬á€•á€«
Â  Â  Â  Â  segments, _ = model.transcribe(audio_path, beam_size=1, vad_filter=True, word_timestamps=True)
Â  Â  Â  Â Â 
Â  Â  Â  Â  # --- SENTENCE RE-GROUPING LOGIC ---
Â  Â  Â  Â  final_subs = []
Â  Â  Â  Â  current_text = []
Â  Â  Â  Â  current_start = None
Â  Â  Â  Â Â 
Â  Â  Â  Â  # á€…á€¬á€€á€¼á€±á€¬á€„á€ºá€¸á€á€…á€ºá€€á€¼á€±á€¬á€„á€ºá€¸ á€¡á€›á€™á€ºá€¸á€›á€¾á€Šá€ºá€™á€á€½á€¬á€¸á€¡á€±á€¬á€„á€º á€‘á€­á€”á€ºá€¸á€–á€­á€¯á€· (Optional)
Â  Â  Â  Â  MAX_CHARS = 100Â 

Â  Â  Â  Â  for segment in segments:
Â  Â  Â  Â  Â  Â  for word in segment.words:
Â  Â  Â  Â  Â  Â  Â  Â  if current_start is None:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  current_start = word.start
Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  # á€…á€€á€¬á€¸á€œá€¯á€¶á€¸á€‘á€Šá€·á€ºá€™á€šá€º
Â  Â  Â  Â  Â  Â  Â  Â  current_text.append(word.word.strip())
Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  # á€…á€…á€ºá€†á€±á€¸á€™á€šá€·á€º á€¡á€á€»á€€á€ºá€™á€»á€¬á€¸:
Â  Â  Â  Â  Â  Â  Â  Â  # á. á€•á€¯á€’á€ºá€™ (. ? !) á€”á€²á€·á€†á€¯á€¶á€¸á€œá€¬á€¸?
Â  Â  Â  Â  Â  Â  Â  Â  # á‚. á€’á€«á€™á€¾á€™á€Ÿá€¯á€á€º á€…á€¬á€œá€¯á€¶á€¸á€›á€± á€¡á€›á€™á€ºá€¸á€™á€»á€¬á€¸á€”á€±á€•á€¼á€®á€œá€¬á€¸? (MAX_CHARS á€€á€»á€±á€¬á€ºá€›á€„á€º á€¡á€á€„á€ºá€¸á€–á€¼á€á€ºá€™á€šá€º)
Â  Â  Â  Â  Â  Â  Â  Â  text_str = " ".join(current_text)
Â  Â  Â  Â  Â  Â  Â  Â  is_end_of_sentence = word.word.strip()[-1] in ".?!"
Â  Â  Â  Â  Â  Â  Â  Â  is_too_long = len(text_str) > MAX_CHARS

Â  Â  Â  Â  Â  Â  Â  Â  if is_end_of_sentence or is_too_long:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  start_ts = format_timestamp(current_start)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  end_ts = format_timestamp(word.end)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  final_subs.append({
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  "start": start_ts,
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  "end": end_ts,
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  "text": text_str
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  })
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # Reset
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  current_text = []
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  current_start = None

Â  Â  Â  Â  # á€€á€»á€”á€ºá€”á€±á€á€²á€·á€á€²á€· á€…á€¬á€™á€»á€¬á€¸á€›á€¾á€­á€›á€„á€º á€”á€±á€¬á€€á€ºá€†á€¯á€¶á€¸ á€…á€¬á€€á€¼á€±á€¬á€„á€ºá€¸á€¡á€–á€¼á€…á€º á€‘á€Šá€·á€ºá€™á€šá€º
Â  Â  Â  Â  if current_text:
Â  Â  Â  Â  Â  Â  # Note: We use the last known word end time, but simplified here
Â  Â  Â  Â  Â  Â  start_ts = format_timestamp(current_start) if current_start else "00:00:00,000"
Â  Â  Â  Â  Â  Â  # Just approximation for end time if strictly needed, or use last word's end
Â  Â  Â  Â  Â  Â  # For robustness, usually we track the last word object.Â 
Â  Â  Â  Â  Â  Â  # But normally the loop handles most.
Â  Â  Â  Â  Â  Â  final_subs.append({
Â  Â  Â  Â  Â  Â  Â  Â  "start": start_ts,
Â  Â  Â  Â  Â  Â  Â  Â  "end": format_timestamp(segments[-1].end), # Fallback to segment end
Â  Â  Â  Â  Â  Â  Â  Â  "text": " ".join(current_text)
Â  Â  Â  Â  Â  Â  })

Â  Â  Â  Â  # File á€‘á€² á€•á€¼á€”á€ºá€›á€±á€¸á€™á€šá€º
Â  Â  Â  Â  with open(srt_path, "w", encoding="utf-8") as srt, open(txt_path, "w", encoding="utf-8") as txt:
Â  Â  Â  Â  Â  Â  for i, sub in enumerate(final_subs, start=1):
Â  Â  Â  Â  Â  Â  Â  Â  srt.write(f"{i}\n{sub['start']} --> {sub['end']}\n{sub['text']}\n\n")
Â  Â  Â  Â  Â  Â  Â  Â  txt.write(f"{sub['text']} ")
Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  return "Whisper (Sentence Mode)"
Â  Â  except Exception as e:
Â  Â  Â  Â  return f"Error: {e}"


def run_gemini_transcribe(audio_path, srt_path, txt_path):
Â  Â  print(f"âœ¨ [Gemini] Listening...")
Â  Â  try:
Â  Â  Â  Â  client = genai.Client(api_key=GEMINI_KEY)
Â  Â  Â  Â  with open(audio_path, "rb") as f: audio_bytes = f.read()
Â  Â  Â  Â  response = client.models.generate_content(
Â  Â  Â  Â  Â  Â  model='gemini-2.0-flash',
Â  Â  Â  Â  Â  Â  contents=[types.Content(parts=[types.Part.from_bytes(data=audio_bytes, mime_type="audio/mp3"), types.Part.from_text(text="Transcribe to text.")])]
Â  Â  Â  Â  )
Â  Â  Â  Â  with open(txt_path, "w", encoding="utf-8") as f: f.write(response.text.strip())
Â  Â  Â  Â  if os.path.exists(srt_path): os.remove(srt_path)Â 
Â  Â  Â  Â  return "Gemini Flash"
Â  Â  except Exception as e:
Â  Â  Â  Â  return "Error"

def format_timestamp(seconds):
Â  Â  hours = math.floor(seconds / 3600)
Â  Â  seconds %= 3600
Â  Â  minutes = math.floor(seconds / 60)
Â  Â  seconds %= 60
Â  Â  milliseconds = round((seconds - math.floor(seconds)) * 1000)
Â  Â  return f"{hours:02}:{minutes:02}:{math.floor(seconds):02},{milliseconds:03}"

# --- ğŸ§  TRANSLATION ---
async def run_translate(user_id, prompt_text):
Â  Â  p = get_paths(user_id)
Â  Â  source_path = p['srt'] if os.path.exists(p['srt']) else p['txt'] if os.path.exists(p['txt']) else None
Â  Â  if not source_path: return False, "âŒ No file found.", None

Â  Â  is_srt = source_path.endswith('.srt')
Â  Â  client = genai.Client(api_key=GEMINI_KEY)
Â  Â Â 
Â  Â  with open(source_path, "r", encoding="utf-8") as f: original_text = f.read()
Â  Â Â 
Â  Â  if is_srt:
Â  Â  Â  Â  ai_prompt = f"{SRT_RULES}\n{prompt_text}\n\n**INPUT SRT:**\n{original_text}"
Â  Â  Â  Â  output_ext = ".srt"
Â  Â  else:
Â  Â  Â  Â  ai_prompt = f"User Instruction: {prompt_text}\n\nInput Text:\n{original_text}"
Â  Â  Â  Â  output_ext = ".txt"
Â  Â Â 
Â  Â  try:
Â  Â  Â  Â  response = client.models.generate_content(model='gemini-2.0-flash', contents=ai_prompt)
Â  Â  Â  Â  content = response.text.strip().replace("```srt", "").replace("```", "").strip()
Â  Â  Â  Â  final_path = p['trans_result'] + output_ext
Â  Â  Â  Â  with open(final_path, "w", encoding="utf-8") as f: f.write(content)
Â  Â  Â  Â  if is_srt: shutil.copy(final_path, p['srt'])Â 
Â  Â  Â  Â  return True, content, final_path
Â  Â  except Exception as e:
Â  Â  Â  Â  return False, str(e), None

# --- ğŸ¤– CHAT GEMINI WITH AUTO-DELETE ---
async def run_chat_gemini(user_id, text):
Â  Â  current_time = time.time()
Â  Â  ONE_DAY_SECONDS = 86400 # 24 Hours

Â  Â  # 1. Check & Auto Delete
Â  Â  # á€¡á€›á€„á€ºá€™á€¾á€á€ºá€‘á€¬á€¸á€á€²á€· á€¡á€á€»á€­á€”á€ºá€›á€¾á€­á€•á€¼á€®á€¸áŠ á€¡á€²á€’á€®á€¡á€á€»á€­á€”á€ºá€€ á‚á„ á€”á€¬á€›á€®á€€á€»á€±á€¬á€ºá€á€½á€¬á€¸á€•á€¼á€®á€†á€­á€¯á€›á€„á€º History á€€á€­á€¯ á€›á€¾á€„á€ºá€¸á€™á€šá€º
Â  Â  if user_id in user_last_active:
Â  Â  Â  Â  if current_time - user_last_active[user_id] > ONE_DAY_SECONDS:
Â  Â  Â  Â  Â  Â  chat_histories[user_id] = [] # Reset History
Â  Â  Â  Â  Â  Â  print(f"ğŸ§¹ Auto-cleared history for {user_id} (Expired)")

Â  Â  # 2. Update Last Active Time
Â  Â  user_last_active[user_id] = current_time

Â  Â  # 3. Normal Chat Process
Â  Â  if user_id not in chat_histories: chat_histories[user_id] = []
Â  Â Â 
Â  Â  client = genai.Client(api_key=GEMINI_KEY)
Â  Â  chat = client.chats.create(model='gemini-2.0-flash', history=chat_histories[user_id])
Â  Â Â 
Â  Â  try:
Â  Â  Â  Â  response = chat.send_message(text)
Â  Â  Â  Â  # History is managed by Gemini object/list automatically in this session
Â  Â  Â  Â  return response.text
Â  Â  except Exception as e:
Â  Â  Â  Â  return f"Gemini Error: {e}"

# --- ğŸ¤– HANDLERS ---
async def post_init(application):
Â  Â  await application.bot.set_my_commands([
Â  Â  Â  Â  BotCommand("start", "ğŸ  Home"),
Â  Â  Â  Â  BotCommand("voices", "ğŸ—£ï¸ Change Voice"),
Â  Â  Â  Â  BotCommand("translate", "ğŸŒ Translate"),
Â  Â  Â  Â  BotCommand("dub", "ğŸ¬ Dub Audio"),
Â  Â  Â  Â  BotCommand("settings", "âš™ï¸ Prompts"),
Â  Â  Â  Â  BotCommand("heygemini", "ğŸ¤– Chat"),
Â  Â  Â  Â  BotCommand("clearall", "ğŸ§¹ Clear"),
Â  Â  Â  Â  BotCommand("cancel", "âŒ Cancel")
Â  Â  ])

async def start(update: Update, context: ContextTypes.DEFAULT_TYPE):
Â  Â  user_id = update.effective_user.id
Â  Â  state = get_user_state(user_id)
Â  Â Â 
Â  Â  voice_name = next((k for k, v in VOICE_LIB.items() if v == state['dub_voice']), "Unknown")
Â  Â  keyboard = [
Â  Â  Â  Â  [InlineKeyboardButton(f"ğŸ™ï¸ Engine: {state['transcribe_engine'].title()}", callback_data="toggle_transcribe")],
Â  Â  Â  Â  [InlineKeyboardButton(f"ğŸ—£ï¸ Voice: {voice_name}", callback_data="cmd_voices")],
Â  Â  Â  Â  [InlineKeyboardButton("âš™ï¸ Settings", callback_data="menu_settings")]
Â  Â  ]
Â  Â  await update.message.reply_text("ğŸ‘‹ **Video AI Studio**\nSend Video, Audio, SRT or TXT.", reply_markup=InlineKeyboardMarkup(keyboard))

async def voices_command(update: Update, context: ContextTypes.DEFAULT_TYPE):
Â  Â  keyboard = []
Â  Â  row = []
Â  Â  for name, code in VOICE_LIB.items():
Â  Â  Â  Â  row.append(InlineKeyboardButton(name, callback_data=f"set_voice_{code}"))
Â  Â  Â  Â  if len(row) == 2:
Â  Â  Â  Â  Â  Â  keyboard.append(row)
Â  Â  Â  Â  Â  Â  row = []
Â  Â  if row: keyboard.append(row)
Â  Â  await update.message.reply_text("ğŸ—£ï¸ **Select Narrator Voice:**", reply_markup=InlineKeyboardMarkup(keyboard))

async def settings_command(update: Update, context: ContextTypes.DEFAULT_TYPE):
Â  Â  keyboard = [
Â  Â  Â  Â  [InlineKeyboardButton("ğŸ“ View Prompts", callback_data="st_view")],
Â  Â  Â  Â  [InlineKeyboardButton("âœï¸ Edit Burmese", callback_data="st_edit_burmese")],
Â  Â  Â  Â  [InlineKeyboardButton("âœï¸ Edit Rephrase", callback_data="st_edit_rephrase")],
Â  Â  Â  Â  [InlineKeyboardButton("ğŸ”„ Reset", callback_data="st_reset")]
Â  Â  ]
Â  Â  await update.message.reply_text("âš™ï¸ **Prompt Settings**", reply_markup=InlineKeyboardMarkup(keyboard))

async def heygemini_command(update: Update, context: ContextTypes.DEFAULT_TYPE):
Â  Â  user_modes[update.effective_user.id] = "chat_gemini"
Â  Â  await update.message.reply_text("ğŸ¤– **Gemini Chat Mode ON**\nType `/cancel` to exit.")

async def clearall_command(update: Update, context: ContextTypes.DEFAULT_TYPE):
Â  Â  wipe_user_data(update.effective_user.id)
Â  Â  await update.message.reply_text("ğŸ§¹ **Cleared.**")

async def cancel_command(update: Update, context: ContextTypes.DEFAULT_TYPE):
Â  Â  user_modes[update.effective_user.id] = None
Â  Â  await update.message.reply_text("âœ… Mode exited.")

async def dub_command(update: Update, context: ContextTypes.DEFAULT_TYPE):
Â  Â  await perform_dubbing(update, context)

async def perform_dubbing(update, context):
Â  Â  user_id = update.effective_user.id
Â  Â  p = get_paths(user_id)
Â  Â  state = get_user_state(user_id)
Â  Â  msg = update.effective_message

Â  Â  if not os.path.exists(p['srt']):
Â  Â  Â  Â  await msg.reply_text("âŒ **No SRT found.**")
Â  Â  Â  Â  return

Â  Â  voice_name = next((k for k, v in VOICE_LIB.items() if v == state['dub_voice']), "Selected Voice")
Â  Â  status = await msg.reply_text(f"ğŸ¬ **Dubbing ({voice_name})...**")
Â  Â Â 
Â  Â  success, error = await generate_dubbing(user_id, p['srt'], p['dub_audio'], state['dub_voice'])
Â  Â Â 
Â  Â  if success:
Â  Â  Â  Â  await status.delete()
Â  Â  Â  Â  await context.bot.send_audio(chat_id=msg.chat_id, audio=open(p['dub_audio'], "rb"), caption=f"âœ… **Dubbed by {voice_name}!**")
Â  Â  else:
Â  Â  Â  Â  await status.edit_text(f"âŒ Dubbing Failed: {error}")

async def translate_command(update: Update, context: ContextTypes.DEFAULT_TYPE):
Â  Â  user_id = update.effective_user.id
Â  Â  user_modes[user_id] = NoneÂ 
Â  Â  keyboard = [
Â  Â  Â  Â  [InlineKeyboardButton("ğŸ‡²ğŸ‡² To Burmese", callback_data="trans_burmese")],
Â  Â  Â  Â  [InlineKeyboardButton("ğŸ‡ºğŸ‡¸ Rephrase", callback_data="trans_rephrase")],
Â  Â  Â  Â  [InlineKeyboardButton("âœï¸ Custom", callback_data="trans_custom")]
Â  Â  ]
Â  Â  await update.message.reply_text("ğŸŒ **Translate Action:**", reply_markup=InlineKeyboardMarkup(keyboard))

async def perform_translation(update, context, user_id, prompt):
Â  Â  msg = update.effective_message
Â  Â  status = await msg.reply_text(f"ğŸŒ **Translating...**")
Â  Â  success, _, path = await run_translate(user_id, prompt)
Â  Â Â 
Â  Â  if success:
Â  Â  Â  Â  await status.delete()
Â  Â  Â  Â  await context.bot.send_document(msg.chat_id, document=open(path, "rb"), caption="âœ… **Translation Done.**")
Â  Â  Â  Â Â 
Â  Â  Â  Â  keyboard = [
Â  Â  Â  Â  Â  Â  [InlineKeyboardButton("âœ… Good", callback_data="feedback_yes"), InlineKeyboardButton("âŒ Bad", callback_data="feedback_no")]
Â  Â  Â  Â  ]
Â  Â  Â  Â  await context.bot.send_message(msg.chat_id, "Translation okay?", reply_markup=InlineKeyboardMarkup(keyboard))
Â  Â  else:
Â  Â  Â  Â  await status.edit_text("âŒ Error.")

async def callback_handler(update: Update, context: ContextTypes.DEFAULT_TYPE):
Â  Â  query = update.callback_query
Â  Â  user_id = query.from_user.id
Â  Â  state = get_user_state(user_id)
Â  Â  data = query.data
Â  Â Â 
Â  Â  if data == "toggle_transcribe":
Â  Â  Â  Â  state['transcribe_engine'] = "gemini" if state['transcribe_engine'] == "whisper" else "whisper"
Â  Â  Â  Â  await query.answer(f"Engine: {state['transcribe_engine']}")
Â  Â Â 
Â  Â  elif data == "cmd_voices":
Â  Â  Â  Â  await voices_command(query, context)
Â  Â  Â  Â  await query.answer()

Â  Â  elif data.startswith("set_voice_"):
Â  Â  Â  Â  new_voice = data.replace("set_voice_", "")
Â  Â  Â  Â  state['dub_voice'] = new_voice
Â  Â  Â  Â  v_name = next((k for k, v in VOICE_LIB.items() if v == new_voice), "Custom Voice")
Â  Â  Â  Â  await query.message.edit_text(f"âœ… Voice set to: **{v_name}**")

Â  Â  elif data == "menu_settings":
Â  Â  Â  Â  await settings_command(update, context)

Â  Â  elif data == "st_view":
Â  Â  Â  Â  await send_copyable_message(query.message.chat_id, context.bot, f"ğŸ‡²ğŸ‡² **Burmese:**\n{get_active_prompt(user_id, 'burmese')}")
Â  Â  Â  Â  await send_copyable_message(query.message.chat_id, context.bot, f"ğŸ‡ºğŸ‡¸ **Rephrase:**\n{get_active_prompt(user_id, 'rephrase')}")

Â  Â  elif data == "st_edit_burmese":
Â  Â  Â  Â  user_modes[user_id] = "edit_prompt_burmese"
Â  Â  Â  Â  await query.message.edit_text("âœï¸ Send new Burmese prompt:")

Â  Â  elif data == "st_edit_rephrase":
Â  Â  Â  Â  user_modes[user_id] = "edit_prompt_rephrase"
Â  Â  Â  Â  await query.message.edit_text("âœï¸ Send new Rephrase prompt:")

Â  Â  elif data == "st_reset":
Â  Â  Â  Â  state['custom_prompts'] = {}
Â  Â  Â  Â  await query.answer("Prompts Reset")

Â  Â  elif data == "trans_burmese":
Â  Â  Â  Â  await perform_translation(update, context, user_id, get_active_prompt(user_id, "burmese"))

Â  Â  elif data == "trans_rephrase":
Â  Â  Â  Â  await perform_translation(update, context, user_id, get_active_prompt(user_id, "rephrase"))

Â  Â  elif data == "trans_custom":
Â  Â  Â  Â  user_modes[user_id] = "translate_prompt"
Â  Â  Â  Â  await query.message.reply_text("âœï¸ Enter custom prompt:")

Â  Â  elif data == "feedback_yes":
Â  Â  Â  Â  p = get_paths(user_id)
Â  Â  Â  Â  if os.path.exists(p['srt']):
Â  Â  Â  Â  Â  Â  keyboard = [[InlineKeyboardButton("ğŸ¬ Make Audio Now", callback_data="trigger_dub")]]
Â  Â  Â  Â  Â  Â  await query.message.edit_text("âœ… Great! Want dubbing?", reply_markup=InlineKeyboardMarkup(keyboard))
Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  await query.message.edit_text("âœ… Thanks!")

Â  Â  elif data == "feedback_no":
Â  Â  Â  Â  user_modes[user_id] = "translate_prompt"
Â  Â  Â  Â  await query.message.edit_text("âœï¸ How should I fix it? (Enter Prompt):")

Â  Â  elif data == "trigger_dub":
Â  Â  Â  Â  await perform_dubbing(update, context)

async def text_handler(update: Update, context: ContextTypes.DEFAULT_TYPE):
Â  Â  msg = update.message
Â  Â  user_id = msg.from_user.id
Â  Â  text = msg.text
Â  Â  mode = user_modes.get(user_id)
Â  Â  state = get_user_state(user_id)
Â  Â  p = get_paths(user_id)

Â  Â  if text.startswith("/cancel"):
Â  Â  Â  Â  await cancel_command(update, context)
Â  Â  Â  Â  return

Â  Â  # --- 1. SRT DETECTION (For Direct Pasting) ---
Â  Â  if re.search(r'\d{2}:\d{2}:\d{2},\d{3} -->', text):
Â  Â  Â  Â  file_mode = 'a'
Â  Â  Â  Â  if re.match(r'^\s*1\s*$', text.split('\n')[0].strip()) or text.strip().startswith('1\n'):
Â  Â  Â  Â  Â  Â  file_mode = 'w'
Â  Â  Â  Â Â 
Â  Â  Â  Â  with open(p['srt'], file_mode, encoding="utf-8") as f: f.write(text + "\n")
Â  Â  Â  Â Â 
Â  Â  Â  Â  keyboard = [[InlineKeyboardButton("ğŸ¬ Dub Audio", callback_data="trigger_dub")]]
Â  Â  Â  Â  await msg.reply_text("âœ… **SRT Text Detected!**\nSaved. Want to dub?", reply_markup=InlineKeyboardMarkup(keyboard))
Â  Â  Â  Â  return

Â  Â  # --- 2. Chat & Settings Modes ---
Â  Â  if mode == "chat_gemini":
Â  Â  Â  Â  await context.bot.send_chat_action(msg.chat_id, "typing")
Â  Â  Â  Â  response = await run_chat_gemini(user_id, text)
Â  Â  Â  Â  await send_copyable_message(msg.chat_id, context.bot, response)
Â  Â  Â  Â  return

Â  Â  if mode == "edit_prompt_burmese":
Â  Â  Â  Â  state.setdefault('custom_prompts', {})['burmese'] = text
Â  Â  Â  Â  user_modes[user_id] = None
Â  Â  Â  Â  await msg.reply_text("âœ… Burmese Prompt Updated.")
Â  Â  Â  Â  return

Â  Â  if mode == "edit_prompt_rephrase":
Â  Â  Â  Â  state.setdefault('custom_prompts', {})['rephrase'] = text
Â  Â  Â  Â  user_modes[user_id] = None
Â  Â  Â  Â  await msg.reply_text("âœ… Rephrase Prompt Updated.")
Â  Â  Â  Â  return

Â  Â  if mode == "translate_prompt":
Â  Â  Â  Â  user_modes[user_id] = None
Â  Â  Â  Â  await perform_translation(update, context, user_id, text)
Â  Â  Â  Â  return

Â  Â  if "http" in text:
Â  Â  Â  Â  await process_media(update, context, is_url=True)
Â  Â  Â  Â  return

Â  Â  # --- 3. Default Text Save ---
Â  Â  if len(text) > 5:
Â  Â  Â  Â  with open(p['txt'], "w", encoding="utf-8") as f: f.write(text)
Â  Â  Â  Â  await msg.reply_text("âœ… Text Saved. Type `/translate`.")

async def file_handler(update: Update, context: ContextTypes.DEFAULT_TYPE):
Â  Â  msg = update.message
Â  Â  user_id = msg.from_user.id
Â  Â  p = get_paths(user_id)
Â  Â  file_obj = await (msg.document or msg.video or msg.audio).get_file()
Â  Â  name = msg.document.file_name if msg.document else "vid.mp4"
Â  Â Â 
Â  Â  if name.lower().endswith('.srt'):
Â  Â  Â  Â  await msg.reply_text("â¬‡ï¸ **SRT Received.**")
Â  Â  Â  Â  await file_obj.download_to_drive(p['srt'])
Â  Â  Â  Â  keyboard = [[InlineKeyboardButton("ğŸ¬ Dub Audio", callback_data="trigger_dub")]]
Â  Â  Â  Â  await msg.reply_text("âœ… **SRT Loaded.**", reply_markup=InlineKeyboardMarkup(keyboard))
Â  Â  Â  Â  return

Â  Â  if name.lower().endswith('.txt'):
Â  Â  Â  Â  await file_obj.download_to_drive(p['txt'])
Â  Â  Â  Â  await msg.reply_text("âœ… **Text Loaded.** Type `/translate`.")
Â  Â  Â  Â  return
Â  Â  Â  Â Â 
Â  Â  await process_media(update, context, is_url=False)

async def process_media(update, context, is_url):
Â  Â  msg = update.message
Â  Â  user_id = msg.from_user.id
Â  Â  p = get_paths(user_id)
Â  Â  state = get_user_state(user_id)
Â  Â Â 
Â  Â  status = await msg.reply_text("â³ **Processing...**")
Â  Â  try:
Â  Â  Â  Â  clean_temp(user_id)
Â  Â  Â  Â  if is_url:
Â  Â  Â  Â  Â  Â  subprocess.run(f"yt-dlp -x --audio-format mp3 -o '{p['audio']}' {msg.text}", shell=True)
Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  file_obj = await (msg.video or msg.document or msg.audio).get_file()
Â  Â  Â  Â  Â  Â  await file_obj.download_to_drive(p['input'])
Â  Â  Â  Â  Â  Â  subprocess.run(f"ffmpeg -y -i {p['input']} -vn -acodec libmp3lame -q:a 2 {p['audio']}", shell=True)
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  loop = asyncio.get_event_loop()
Â  Â  Â  Â  if state['transcribe_engine'] == "whisper":
Â  Â  Â  Â  Â  Â  await loop.run_in_executor(None, run_whisper, p['audio'], p['srt'], p['txt'])
Â  Â  Â  Â  Â  Â  if os.path.exists(p['srt']):
Â  Â  Â  Â  Â  Â  Â  Â  await context.bot.send_document(msg.chat_id, open(p['srt'], "rb"), caption="ğŸ¬ Subtitles")
Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  await loop.run_in_executor(None, run_gemini_transcribe, p['audio'], p['srt'], p['txt'])
Â  Â  Â  Â  Â  Â  if os.path.exists(p['txt']):
Â  Â  Â  Â  Â  Â  Â  Â  Â await context.bot.send_document(msg.chat_id, open(p['txt'], "rb"), caption="ğŸ“„ Transcript")
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  await status.edit_text("âœ… **Done!** Type `/translate` or `/dub`.")

Â  Â  except Exception as e:
Â  Â  Â  Â  await status.edit_text(f"âŒ Error: {e}")

if __name__ == '__main__':
Â  Â  print("ğŸš€ Video AI Bot Running...")
Â  Â  app = ApplicationBuilder().token(TG_TOKEN).post_init(post_init).build()
Â  Â  app.add_handler(CommandHandler("start", start))
Â  Â  app.add_handler(CommandHandler("voices", voices_command))
Â  Â  app.add_handler(CommandHandler("settings", settings_command))
Â  Â  app.add_handler(CommandHandler("heygemini", heygemini_command))
Â  Â  app.add_handler(CommandHandler("translate", translate_command))
Â  Â  app.add_handler(CommandHandler("dub", dub_command))
Â  Â  app.add_handler(CommandHandler("clearall", clearall_command))
Â  Â  app.add_handler(CommandHandler("cancel", cancel_command))
Â  Â  app.add_handler(CallbackQueryHandler(callback_handler))
Â  Â  app.add_handler(MessageHandler(filters.TEXT & (~filters.COMMAND), text_handler))
Â  Â  app.add_handler(MessageHandler(filters.VIDEO | filters.Document.ALL | filters.AUDIO, file_handler))
Â  Â  app.run_polling()